{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mx9LoRqF6sz_",
        "3lfrB_rcABiC",
        "pN3gs4gR6wSZ",
        "gqcS0S1w-SNw",
        "be843t90lfB_",
        "NVg6D1NXgU2y",
        "-uIkqaOV44To",
        "OJhzBZwgLMYv"
      ],
      "mount_file_id": "19TbuL2ujRBzc8NjkMMfD_laerig8t3eb",
      "authorship_tag": "ABX9TyMIhbl1JM55rxsL3gQhOAT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcroysdill-dot/CMET/blob/main/CMET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CMET .mat file processing"
      ],
      "metadata": {
        "id": "mx9LoRqF6sz_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6uY43mzTDLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YD0iN6ZTDJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# A function to explore the raw matlab files downloaded\n",
        "########################\n",
        "\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "def explore_mat_file(filepath):\n",
        "    \"\"\"\n",
        "    Load and explore a .mat file with a structured array under key 'f'.\n",
        "    Prints available fields and shape of each field's data.\n",
        "    \"\"\"\n",
        "    print(f\"Loading file: {filepath}\")\n",
        "    data = loadmat(filepath)\n",
        "\n",
        "    # List all keys in the .mat file\n",
        "    print(\"Top-level keys in the .mat file:\")\n",
        "    print(data.keys())\n",
        "    print()\n",
        "\n",
        "    if 'f' not in data:\n",
        "        print(\"Warning: Key 'f' not found in the .mat file.\")\n",
        "        return\n",
        "\n",
        "    f = data['f']\n",
        "    print(f\"Type of 'f': {type(f)}\")\n",
        "    print(f\"Shape of 'f': {f.shape}\")\n",
        "    print()\n",
        "\n",
        "    # Check if 'f' is a structured array with named fields\n",
        "    if not hasattr(f.dtype, 'names') or f.dtype.names is None:\n",
        "        print(\"'f' does not appear to be a structured array with named fields.\")\n",
        "        return\n",
        "\n",
        "    fields = f.dtype.names\n",
        "    print(f\"Fields found in 'f': {fields}\")\n",
        "    print()\n",
        "\n",
        "    # Since 'f' is likely a 2D array with one element, extract that element\n",
        "    f0 = f[0, 0]\n",
        "\n",
        "    # Print shape of arrays stored in each field\n",
        "    for field in fields:\n",
        "        try:\n",
        "            arr = f0[field]\n",
        "            # Some fields might be scalars, some arrays\n",
        "            if isinstance(arr, np.ndarray):\n",
        "                print(f\"{field}: shape {arr.shape}, dtype {arr.dtype}\")\n",
        "            else:\n",
        "                print(f\"{field}: type {type(arr)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not access field '{field}': {e}\")\n",
        "\n",
        "    print(\"\\nExample access:\")\n",
        "    # Example: safely access lat, lon, alt if they exist\n",
        "    for key in ['lat', 'lon', 'alt']:\n",
        "        if key in fields:\n",
        "            try:\n",
        "                val = f0[key]\n",
        "                print(f\"{key} sample value (shape {val.shape}):\")\n",
        "                print(val)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not access {key}: {e}\")\n",
        "        else:\n",
        "            print(f\"Field '{key}' not found in 'f'.\")\n",
        "\n",
        "# Example usage:\n",
        "explore_mat_file(\"/content/drive/MyDrive/MScThesis/from_mat_files/170112a.mat\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GVe0K9Ln8bJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# Converting the .mat files into csv's\n",
        "# NOTE: This saves the file in the temporary storage so you have to move it\n",
        "# NOTE: I created this because most variables have a value for each observational\n",
        "#       point but some (like CMD or id only have one or a few so they are separated\n",
        "#       into arrays_1d and scalars)\n",
        "########################\n",
        "\n",
        "!pip install pymatreader\n",
        "\n",
        "from pymatreader import read_mat\n",
        "\n",
        "data = read_mat(\"/content/drive/MyDrive/MScThesis/from_mat_files/170112a.mat\")\n",
        "f = data['f']  # Your data dictionary\n",
        "\n",
        "arrays_1d = {}\n",
        "scalars = {}\n",
        "\n",
        "# Detect all 1D arrays and their lengths\n",
        "lengths = set()\n",
        "\n",
        "for key, val in f.items():\n",
        "    if isinstance(val, np.ndarray):\n",
        "        if val.ndim == 1:\n",
        "            lengths.add(val.shape[0])\n",
        "            arrays_1d[key] = val\n",
        "        else:\n",
        "            scalars[key] = val\n",
        "    else:\n",
        "        scalars[key] = val\n",
        "\n",
        "# Check if all 1D arrays have the same length\n",
        "if len(lengths) == 1:\n",
        "    array_length = lengths.pop()\n",
        "    print(f\"Detected 1D arrays all have length: {array_length}\")\n",
        "else:\n",
        "    print(f\"Warning: Found 1D arrays of different lengths: {lengths}\")\n",
        "    # You can decide how to handle this case:\n",
        "    # For now, only keep arrays with the most common length\n",
        "    from collections import Counter\n",
        "    length_counts = Counter([val.shape[0] for val in arrays_1d.values()])\n",
        "    most_common_length = length_counts.most_common(1)[0][0]\n",
        "    print(f\"Using arrays of length {most_common_length} for CSV\")\n",
        "    arrays_1d = {k: v for k, v in arrays_1d.items() if v.shape[0] == most_common_length}\n",
        "    array_length = most_common_length\n",
        "\n",
        "# --- Save arrays CSV ---\n",
        "\n",
        "df_arrays = pd.DataFrame(arrays_1d)\n",
        "df_arrays.to_csv('arrays_1d.csv', index=False)\n",
        "print(\"Saved arrays_1d.csv\")\n",
        "\n",
        "# --- Prepare scalars CSV in long format ---\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Handle CMD separately if present (flatten and save each element as a row)\n",
        "if 'CMD' in scalars:\n",
        "    cmd_array = scalars.pop('CMD')\n",
        "    cmd_flat = cmd_array.flatten()\n",
        "    for i, v in enumerate(cmd_flat):\n",
        "        rows.append({'variable': f'CMD_{i}', 'value': v})\n",
        "\n",
        "for key, val in scalars.items():\n",
        "    if isinstance(val, np.ndarray):\n",
        "        if val.size == 1:\n",
        "            rows.append({'variable': key, 'value': val.item()})\n",
        "        else:\n",
        "            flat = val.flatten()\n",
        "            for i, v in enumerate(flat):\n",
        "                rows.append({'variable': f'{key}_{i}', 'value': v})\n",
        "    else:\n",
        "        rows.append({'variable': key, 'value': val})\n",
        "\n",
        "df_scalars = pd.DataFrame(rows)\n",
        "df_scalars.to_csv('scalars_long_format.csv', index=False)\n",
        "print(\"Saved scalars_long_format.csv\")\n"
      ],
      "metadata": {
        "id": "-8a6oRmn4dfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CMET .txt file processing"
      ],
      "metadata": {
        "id": "3lfrB_rcABiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# From raw .txt files\n",
        "# Just viewing the file, checking the structure and NaN's\n",
        "####################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/MScThesis/from_txt_files/170112a.txt\"\n",
        "\n",
        "# Read whitespace-separated file (tabs OR spaces)\n",
        "df = pd.read_csv(\n",
        "    file_path,\n",
        "    sep=r\"\\s+\",        # any whitespace\n",
        "    header=None,       # no header in file\n",
        "    engine=\"python\",\n",
        "    na_values=[\"NaN\"]\n",
        ")\n",
        "\n",
        "print(\"File loaded successfully.\")\n",
        "\n",
        "# Checking size and shape\n",
        "print(\"\\nShape (rows, columns):\", df.shape)\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Checking how many NaN's are present\n",
        "print(\"\\nNaN count per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Check if columns have different data types within\n",
        "for col in df.columns:\n",
        "    print(f\"\\nColumn {col}:\")\n",
        "    print(\"dtype:\", df[col].dtype)\n",
        "    print(\"Python types inside:\")\n",
        "    print(df[col].apply(type).value_counts())\n",
        "\n",
        "print(df.describe()) # gives the statistics of each column and automatically excludes NaN's\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BAC7U4cjFP_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# This file creates a new .csv which adds timestamps to the txt files in lots\n",
        "# of different formats based on the julian day in the first column\n",
        "####################\n",
        "\n",
        "# ===== CONFIG =====\n",
        "file_path = \"/content/drive/MyDrive/MScThesis/from_txt_files/171122a.txt\"   # input text file\n",
        "output_csv = \"171122a_time.csv\" # output CSV\n",
        "year = 2017                       # year to use for Julian day\n",
        "\n",
        "# Read raw file\n",
        "df = pd.read_csv(file_path, sep=r'\\s+', header=None, engine='python')\n",
        "\n",
        "# Convert Julian day to datetime\n",
        "df['datetime'] = pd.to_datetime(df[0] - 1, unit='D', origin=pd.Timestamp(f'{year}-01-01'))\n",
        "\n",
        "# Extract components with nullable integers\n",
        "df['year']   = df['datetime'].dt.year.astype('Int64')\n",
        "df['month']  = df['datetime'].dt.month.astype('Int64')\n",
        "df['day']    = df['datetime'].dt.day.astype('Int64')\n",
        "df['hour']   = df['datetime'].dt.hour.astype('Int64')\n",
        "df['minute'] = df['datetime'].dt.minute.astype('Int64')\n",
        "df['second'] = df['datetime'].dt.second.astype('Int64')\n",
        "\n",
        "# Extra formats\n",
        "df['HHMMSS'] = df['datetime'].dt.strftime('%H%M%S')\n",
        "df['dd-mm']  = df['datetime'].dt.strftime('%d-%m')\n",
        "df['datetime_iso'] = df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f000')\n",
        "\n",
        "# Save to new CSV\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"New CSV saved: {output_csv}\")"
      ],
      "metadata": {
        "id": "HSTAXFSGBdPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ERA5 CDS API Downloads"
      ],
      "metadata": {
        "id": "pN3gs4gR6wSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netCDF4 cartopy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jIEAv76htCF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import netCDF4 # netCDF4: reads .nc files efficiently\n",
        "import xarray  # xarray: essential for ERA5 (lazy loading, metadata)\n",
        "import dask    # dask: chunked, out-of-core processing (critical for large data)\n",
        "import os\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cartopy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tE9s4Xb36v7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir(\"/content/drive/MyDrive/MScThesis/ERA5\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "jmwb8k-eYsRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'url: https://cds.climate.copernicus.eu/api'\n",
        "key = 'key: <insert my key here>'\n",
        "\n",
        "with open('/root/.cdsapirc', 'w') as f:\n",
        "    f.write('\\n'.join([url, key]))\n",
        "\n",
        "with open('/root/.cdsapirc') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "g3Br92fP9iZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"cdsapi>=0.7.7\""
      ],
      "metadata": {
        "id": "9jqZ84Mt9kTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check status of download on the web (after login): https://cds.climate.copernicus.eu/requests?tab=all"
      ],
      "metadata": {
        "id": "IK9Df9U9W2Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "# To fill in before a request: #\n",
        "################################\n",
        "\n",
        "path = \"/content/drive/MyDrive/MScThesis/ERA5\"\n",
        "filename = \"170112a_T_500_1501.nc\"      # <---Must edit before a request (check extension is correct)\n",
        "\n",
        "output_file = os.path.join(path, filename)"
      ],
      "metadata": {
        "id": "aDeGAqhqf0Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cdsapi # don't move this to the top, idk why just don't\n",
        "\n",
        "dataset = \"reanalysis-era5-pressure-levels\"\n",
        "request = {\n",
        "    \"product_type\": [\"reanalysis\"],\n",
        "    \"variable\": [\"temperature\"],\n",
        "    \"year\": [\"2017\"],\n",
        "    \"month\": [\"01\"],\n",
        "    \"day\": [\n",
        "        \"15\"\n",
        "    ],\n",
        "    \"time\": [\n",
        "        \"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\", \"05:00\", \"06:00\",\n",
        "        \"07:00\", \"08:00\", \"09:00\", \"10:00\", \"11:00\", \"12:00\", \"13:00\",\n",
        "        \"14:00\", \"15:00\", \"16:00\", \"17:00\", \"18:00\", \"19:00\", \"20:00\",\n",
        "        \"21:00\", \"22:00\", \"23:00\"\n",
        "    ],\n",
        "    \"pressure_level\": [\"500\"\n",
        "    ],\n",
        "    \"data_format\": \"netcdf\",\n",
        "    \"download_format\": \"unarchived\",\n",
        "    \"area\": [-72, -180, -88, 0]\n",
        "}\n",
        "\n",
        "client = cdsapi.Client()\n",
        "client.retrieve(dataset, request).download(output_file)"
      ],
      "metadata": {
        "id": "ezcm8bum8-Vz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To show metadata and structure on an ERA5 netCDF file\n",
        "import xarray as xr\n",
        "data = xr.open_dataset('/content/drive/MyDrive/MScThesis/ERA5/170112a_T_500_1501.nc')\n",
        "data"
      ],
      "metadata": {
        "id": "OFrwNN2rBSKG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uat95p2NFV9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DZpQ6_fg4dcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6p_s7-M4dVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P7zRjNI1w8Uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X8qNvq9fw-Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mapping"
      ],
      "metadata": {
        "id": "gqcS0S1w-SNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas matplotlib cartopy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m28SMixQbQHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "\n",
        "# ===== STEP 1: Load the weather balloon data =====\n",
        "# First 3 columns: jd, lat, lon\n",
        "# No header, sep could be space or tab\n",
        "balloon_file = \"/content/drive/MyDrive/MScThesis/from_txt_files/170112a.txt\"\n",
        "\n",
        "df_balloon = pd.read_csv(\n",
        "    balloon_file,\n",
        "    sep=r'\\s+',\n",
        "    header=None,\n",
        "    engine='python',\n",
        "    usecols=[0,1,2],\n",
        "    names=['jd','lat','lon']\n",
        ")\n",
        "\n",
        "# ===== STEP 2: Load radiosonde launch data =====\n",
        "radiosonde_file = \"/content/drive/MyDrive/MScThesis/radiosondes_Halley/radisondes_times.csv\"\n",
        "df_launch = pd.read_csv(radiosonde_file)\n",
        "\n",
        "# df_launch should have columns: 'radiosonde', 'jd_start', 'jd_end'\n",
        "\n",
        "# ===== STEP 3: Check which balloon points are within any radiosonde window =====\n",
        "# Create a boolean column for highlight\n",
        "df_balloon['highlight'] = False\n",
        "df_balloon['radiosonde_label'] = ''\n",
        "\n",
        "for idx, row in df_launch.iterrows():\n",
        "    mask = (df_balloon['jd'] >= row['jd_start']) & (df_balloon['jd'] <= row['jd_end'])\n",
        "    df_balloon.loc[mask, 'highlight'] = True\n",
        "    df_balloon.loc[mask, 'radiosonde_label'] = row['radiosonde']\n",
        "\n",
        "# ===== STEP 4: Plot the map =====\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
        "ax.set_extent([-180, 180, -90, -60], ccrs.PlateCarree())  # Antarctica region\n",
        "\n",
        "# Add coastlines and land features\n",
        "ax.add_feature(cfeature.COASTLINE)\n",
        "ax.add_feature(cfeature.LAND, facecolor='lightgrey')\n",
        "ax.gridlines(draw_labels=True)\n",
        "\n",
        "# Plot all balloon points\n",
        "ax.scatter(df_balloon['lon'], df_balloon['lat'], s=10, c='blue', transform=ccrs.PlateCarree(), label='Balloon data')\n",
        "\n",
        "# Highlight points within radiosonde launch windows\n",
        "highlight = df_balloon[df_balloon['highlight']]\n",
        "ax.scatter(highlight['lon'], highlight['lat'], s=30, c='red', transform=ccrs.PlateCarree(), label='Radiosonde window')\n",
        "\n",
        "# Add a single label per radiosonde launch, next to the first highlighted point\n",
        "for rs in df_launch['radiosonde']:\n",
        "    subset = df_balloon[df_balloon['radiosonde_label'] == rs]\n",
        "    if not subset.empty:\n",
        "        first_point = subset.iloc[0]  # pick the first highlighted point\n",
        "        ax.text(\n",
        "            first_point['lon'],\n",
        "            first_point['lat'] + 0.2,  # small vertical offset\n",
        "            rs,\n",
        "            color='red',\n",
        "            fontsize=10,\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            transform=ccrs.PlateCarree()\n",
        "        )\n",
        "\n",
        "# ===== ADD HALLEY STATION =====\n",
        "halley_lat = -75.6111   # 75°36'40\"S\n",
        "halley_lon = -26.2669   # 26°16'1\"W\n",
        "\n",
        "# Plot the station and give it a label for the legend\n",
        "halley_plot = ax.plot(\n",
        "    halley_lon, halley_lat,\n",
        "    marker='*', color='green', markersize=15,\n",
        "    transform=ccrs.PlateCarree(),\n",
        "    label='Halley Station'\n",
        ")[0]  # plot returns a list of Line2D, take first element\n",
        "\n",
        "plt.title(\"CMET 170112a launch against Radiosonde Launch timing\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_YJEqiyWP5Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ===== STEP 1: Load the balloon data =====\n",
        "balloon_file = \"/content/drive/MyDrive/MScThesis/from_txt_files/170115a.txt\"\n",
        "df_balloon = pd.read_csv(\n",
        "    balloon_file,\n",
        "    sep=r'\\s+',\n",
        "    header=None,\n",
        "    engine='python',\n",
        "    usecols=[0,1,2],\n",
        "    names=['jd','lat','lon']\n",
        ")\n",
        "\n",
        "# ===== STEP 2: Convert Julian day to datetime =====\n",
        "# Assuming all JD are in 2017\n",
        "year = 2017\n",
        "# Drop rows with missing JD\n",
        "df_balloon = df_balloon.dropna(subset=['jd'])\n",
        "\n",
        "# Convert JD to float\n",
        "df_balloon['jd'] = df_balloon['jd'].astype(float)\n",
        "\n",
        "# Convert Julian day to datetime (ERA5-style, origin at Jan 1, 2017)\n",
        "year = 2017\n",
        "df_balloon['datetime'] = pd.to_datetime(df_balloon['jd'] - 1, unit='D', origin=pd.Timestamp(f'{year}-01-01'))\n",
        "\n",
        "# ===== STEP 3: Find points at midnight =====\n",
        "# Midnight is when hour == 0 and minute == 0\n",
        "df_midnight = df_balloon[df_balloon['datetime'].dt.hour == 0]\n",
        "# Alternatively, if no hourly info, pick the first point of each integer JD:\n",
        "df_midnight = df_balloon[df_balloon['jd'].astype(int).diff() != 0]  # first point of each day\n",
        "\n",
        "# ===== STEP 4: Plot map =====\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
        "ax.set_extent([-180, 180, -90, -60], ccrs.PlateCarree())\n",
        "\n",
        "# Add features\n",
        "ax.add_feature(cfeature.COASTLINE)\n",
        "ax.add_feature(cfeature.LAND, facecolor='lightgrey')\n",
        "ax.gridlines(draw_labels=True)\n",
        "\n",
        "# Plot full balloon path\n",
        "ax.plot(df_balloon['lon'], df_balloon['lat'], c='blue', transform=ccrs.PlateCarree(), label='Balloon path')\n",
        "\n",
        "# Highlight midnight points\n",
        "ax.scatter(df_midnight['lon'], df_midnight['lat'], c='red', s=50, transform=ccrs.PlateCarree(), label='Midnight points')\n",
        "\n",
        "# Label each midnight point with DD-MM-2017\n",
        "for idx, row in df_midnight.iterrows():\n",
        "    label_text = row['datetime'].strftime('%d-%m-2017')\n",
        "    ax.text(row['lon'], row['lat'], label_text, color='red', fontsize=9,\n",
        "            ha='center', va='bottom', transform=ccrs.PlateCarree(), clip_on=False)\n",
        "\n",
        "plt.title(\"170115a CMET Balloon Path\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N30lW-3aP5Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k50zNBuHP49o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting"
      ],
      "metadata": {
        "id": "be843t90lfB_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrx7xNwtlhtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8o9SdrJlhki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OVaxM3vAlhPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Radiosonde data processing"
      ],
      "metadata": {
        "id": "NVg6D1NXgU2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# adding time infomation to Halley radiosonde launches\n",
        "# all in UTC\n",
        "####################\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# ===== CONFIG =====\n",
        "file_path = \"/content/drive/MyDrive/MScThesis/radiosondes_Halley/raw_dat_files/20170119.dat\"\n",
        "output_csv = \"20170119_time.csv\"\n",
        "\n",
        "# ===== STEP 1: Read the file =====\n",
        "# Read first line separately (launch info)\n",
        "with open(file_path, 'r') as f:\n",
        "    first_line = f.readline().strip()\n",
        "\n",
        "# Extract launch info: Year, Month, Day, Hour, Minute\n",
        "launch_info = first_line.split()\n",
        "launch_year   = int(launch_info[0])\n",
        "launch_month  = int(launch_info[1])\n",
        "launch_day    = int(launch_info[2])\n",
        "launch_hour   = int(launch_info[3])\n",
        "launch_minute = int(launch_info[4])\n",
        "# ignore the 6th field\n",
        "\n",
        "launch_datetime = pd.Timestamp(year=launch_year,\n",
        "                               month=launch_month,\n",
        "                               day=launch_day,\n",
        "                               hour=launch_hour,\n",
        "                               minute=launch_minute)\n",
        "\n",
        "# ===== STEP 2: Read the rest of the file =====\n",
        "# 2nd row onwards — all data\n",
        "# There are 9 columns: Minutes, Seconds, Pressure, Height, Temp, Humidity, Dewpoint, WindDir, WindSpd\n",
        "df = pd.read_csv(file_path, sep=r'\\s+', header=None, skiprows=1,\n",
        "                 names=['minutes_into_flight', 'seconds_into_flight', 'pressure',\n",
        "                        'height', 'temperature', 'humidity', 'dewpoint',\n",
        "                        'wind_dir', 'wind_speed'],\n",
        "                 engine='python')\n",
        "\n",
        "# ===== STEP 3: Compute absolute datetime for each row =====\n",
        "df['datetime'] = df.apply(\n",
        "    lambda row: launch_datetime + timedelta(minutes=row['minutes_into_flight'],\n",
        "                                            seconds=row['seconds_into_flight']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# ===== STEP 4: Extract extra columns =====\n",
        "# Year, month, day, hour, minute, second\n",
        "df['year']   = df['datetime'].dt.year\n",
        "df['month']  = df['datetime'].dt.month\n",
        "df['day']    = df['datetime'].dt.day\n",
        "df['hour']   = df['datetime'].dt.hour\n",
        "df['minute'] = df['datetime'].dt.minute\n",
        "df['second'] = df['datetime'].dt.second\n",
        "# HHMMSS with leading zeros\n",
        "df['HHMMSS'] = df['datetime'].apply(lambda x: f\"{x.hour:02d}{x.minute:02d}{x.second:02d}\")\n",
        "# dd-mm\n",
        "df['dd-mm'] = df['datetime'].dt.strftime('%d-%m')\n",
        "# Optional ISO format\n",
        "df['datetime_iso'] = df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f000')\n",
        "# Decimal Julian day\n",
        "df['julian_day_decimal'] = df['datetime'].apply(\n",
        "    lambda x: x.dayofyear + (x.hour*3600 + x.minute*60 + x.second)/86400)\n",
        "\n",
        "# ===== STEP 5: Save to CSV =====\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"New CSV saved: {output_csv}\")"
      ],
      "metadata": {
        "id": "vwIUjeXMBdFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other analyses that I could use again"
      ],
      "metadata": {
        "id": "-uIkqaOV44To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A script to match the closest AMPS meteogram station to the location of each data point in the .kml file (which I manually converted into a csv. Already done for the 170 flights. AMPS meteogram stations converted into a csv using code in the grave)"
      ],
      "metadata": {
        "id": "PQz8zZwxQAsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "# Matching the closest AMPS meteogram station in domain 2 to points along the trajectory\n",
        "# Note that the csv files of the balloons are created from the .kml files\n",
        "# ^ This is done manually by opening the file from my folder and copying the relevant data\n",
        "#   into a text file then saving as a csv\n",
        "##################\n",
        "\n",
        "import csv\n",
        "import math\n",
        "\n",
        "def haversine(lon1, lat1, lon2, lat2):\n",
        "    # Calculate the great circle distance between two points on the earth (in kilometers)\n",
        "    R = 6371  # Earth radius in km\n",
        "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.asin(math.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "# Load meteogram stations\n",
        "\n",
        "CMET = \"/content/drive/MyDrive/MScThesis/from_mat_files/170115a.csv\"\n",
        "AMPS_stations = \"/content/drive/MyDrive/MScThesis/AMPS/AMPS_domain2_meteogram_sites.csv\"\n",
        "output = \"/content/drive/MyDrive/MScThesis/AMPS/170115a_withAMPSstations.csv\"\n",
        "\n",
        "stations = []\n",
        "with open(AMPS_stations, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        stations.append({\n",
        "            'abbrev': row['abbrev'],\n",
        "            'lat': float(row['latitude']),\n",
        "            'lon': float(row['longitude'])\n",
        "        })\n",
        "\n",
        "\n",
        "# Process points and find closest station\n",
        "with open(CMET, 'r') as infile, open(output, 'w', newline='') as outfile:\n",
        "    reader = csv.DictReader(infile)\n",
        "    fieldnames = reader.fieldnames + ['AMPS_meteogram_stn']\n",
        "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for row in reader:\n",
        "        point_lon = float(row['lon'])\n",
        "        point_lat = float(row['lat'])\n",
        "\n",
        "        # Find closest station\n",
        "        closest_station = None\n",
        "        min_distance = float('inf')\n",
        "        for st in stations:\n",
        "            dist = haversine(point_lon, point_lat, st['lon'], st['lat'])\n",
        "            if dist < min_distance:\n",
        "                min_distance = dist\n",
        "                closest_station = st['abbrev']\n",
        "\n",
        "        # Add closest station abbrev to row\n",
        "        row['AMPS_meteogram_stn'] = closest_station\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "6q5D36uprCRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grave"
      ],
      "metadata": {
        "id": "OJhzBZwgLMYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "# Converting radiosonde .dat files from Halley Station into .csv\n",
        "# Data from BAS (see email from Steve Colwell)\n",
        "#########\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload multiple .dat files\n",
        "print(\"Upload your tab-delimited .dat files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Loop through each uploaded file and convert to .csv\n",
        "for filename in uploaded.keys():\n",
        "    try:\n",
        "        # Read the .dat file as tab-delimited\n",
        "        df = pd.read_csv(filename, delimiter='\\t')\n",
        "\n",
        "        # Create the .csv filename\n",
        "        csv_filename = filename.rsplit('.', 1)[0] + '.csv'\n",
        "\n",
        "        # Save as .csv\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "\n",
        "        print(f\"Converted {filename} to {csv_filename}\")\n",
        "\n",
        "        # Step 3: Download the .csv file\n",
        "        files.download(csv_filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert {filename}: {e}\")\n"
      ],
      "metadata": {
        "id": "Dul0z13M2Mjo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "# Converting a text file of AMPS meteogram sites into csv\n",
        "# Data from: https://www2.mmm.ucar.edu/rt/amps/information/meteogram_sites/index.html\n",
        "#########\n",
        "\n",
        "\"\"\" Example:\n",
        " abbrev | site name                      | latitude | longitude\n",
        "--------+--------------------------------+----------+-----------\n",
        " abn    | ABN                            | -71.1667 |  111.4000\n",
        " aboa   | DML_Wasa_Aboa                  | -73.0300 |  -13.4100\n",
        " adare  | USAP_Cape_Adare                | -71.2833 |  170.2330\n",
        " \"\"\"\n",
        "\n",
        "import csv\n",
        "\n",
        "input_file  = '/content/drive/MyDrive/MScThesis/AMPS/AMPS_domain2_meteogram_sites.txt'\n",
        "output_file = '/content/drive/MyDrive/MScThesis/AMPS/AMPS_domain2_meteogram_sites.csv'\n",
        "\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        # Skip empty lines or separator lines\n",
        "        if not line or set(line) <= set('-+'):\n",
        "            continue\n",
        "\n",
        "        # Split by pipe and strip whitespace\n",
        "        fields = [field.strip() for field in line.split('|')]\n",
        "\n",
        "        # Write header or data rows\n",
        "        writer.writerow(fields)\n",
        "\n",
        "print(f\"Conversion complete! CSV saved as {output_file}\")\n",
        "\n",
        "###########\n",
        "# Then I am converting this into KML to add it to my google earth map for visualisation\n",
        "###########\n",
        "\n",
        "input_file = output_file  # CSV file generated from your text file\n",
        "kml_file = '/content/drive/MyDrive/MScThesis/AMPS/AMPS_domain2_meteogram_sites.kml'\n",
        "\n",
        "# KML file header and footer templates\n",
        "kml_header = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<kml xmlns=\"http://www.opengis.net/kml/2.2\">\n",
        "  <Document>\n",
        "    <name>Sites Abbreviations</name>\n",
        "'''\n",
        "\n",
        "kml_footer = '''  </Document>\n",
        "</kml>'''\n",
        "\n",
        "with open(input_file, 'r') as csvfile, open(kml_file, 'w') as kml:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    kml.write(kml_header)\n",
        "\n",
        "    for row in reader:\n",
        "        abbrev = row['abbrev']\n",
        "        lat = row['latitude']\n",
        "        lon = row['longitude']\n",
        "\n",
        "        # Write a Placemark for each site with abbreviation as the label\n",
        "        placemark = f'''\n",
        "    <Placemark>\n",
        "      <name>{abbrev}</name>\n",
        "      <Point>\n",
        "        <coordinates>{lon},{lat},0</coordinates>\n",
        "      </Point>\n",
        "    </Placemark>\n",
        "'''\n",
        "        kml.write(placemark)\n",
        "\n",
        "    kml.write(kml_footer)\n",
        "\n",
        "print(f\"KML file created: {kml_file}\")"
      ],
      "metadata": {
        "id": "LsscYGFE1uHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install netCDF4 xarray"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iTkWRQNEkv8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "# This was just to read sea-level pressure anomaly data from Antarctica in January 2017\n",
        "# from: https://data.seaiceportal.de/relaunch/SLP.php?region=s&minYear=2017&minMonth=1&maxYear=2017&maxMonth=1&submit1=New+search&lang=en&active-tab2=clima&active-tab1=derived_products&ice-type=clima&resolution=monthly&parameter=SLP\n",
        "#\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "# Open the .nc file\n",
        "dataset = Dataset('/content/drive/MyDrive/MScThesis/slp_201701.nc', 'r')\n",
        "\n",
        "# Print the dataset structure\n",
        "print(dataset)\n",
        "\n",
        "# Access variables\n",
        "print(dataset.variables.keys())\n",
        "\n",
        "# Read a variable, e.g. temperature\n",
        "slp = dataset.variables['slp'][:]\n",
        "\n",
        "print(slp)\n",
        "\n",
        "# Close the dataset\n",
        "dataset.close()\n"
      ],
      "metadata": {
        "id": "Q3wBCQTOkzxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}